{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('madelon/train-X.npy')\n",
    "y_train = np.load('madelon/train-y.npy')\n",
    "X_test = np.load('madelon/test-X.npy')\n",
    "y_test = np.load('madelon/test-y.npy')\n",
    "\n",
    "train_X0 = np.load('madelon/cv-train-X.0.npy')\n",
    "train_X1 = np.load('madelon/cv-train-X.1.npy')\n",
    "train_X2 = np.load('madelon/cv-train-X.2.npy')\n",
    "train_X3 = np.load('madelon/cv-train-X.3.npy')\n",
    "train_X4 = np.load('madelon/cv-train-X.4.npy')\n",
    "\n",
    "train_y0 = np.load('madelon/cv-train-y.0.npy')\n",
    "train_y1 = np.load('madelon/cv-train-y.1.npy')\n",
    "train_y2 = np.load('madelon/cv-train-y.2.npy')\n",
    "train_y3 = np.load('madelon/cv-train-y.3.npy')\n",
    "train_y4 = np.load('madelon/cv-train-y.4.npy')\n",
    "\n",
    "heldout_X0 = np.load('madelon/cv-heldout-X.0.npy')\n",
    "heldout_X1 = np.load('madelon/cv-heldout-X.1.npy')\n",
    "heldout_X2 = np.load('madelon/cv-heldout-X.2.npy')\n",
    "heldout_X3 = np.load('madelon/cv-heldout-X.3.npy')\n",
    "heldout_X4 = np.load('madelon/cv-heldout-X.4.npy')\n",
    "\n",
    "heldout_y0 = np.load('madelon/cv-heldout-y.0.npy')\n",
    "heldout_y1 = np.load('madelon/cv-heldout-y.1.npy')\n",
    "heldout_y2 = np.load('madelon/cv-heldout-y.2.npy')\n",
    "heldout_y3 = np.load('madelon/cv-heldout-y.3.npy')\n",
    "heldout_y4 = np.load('madelon/cv-heldout-y.4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2(a) Training Classifiers\n",
    "# Algorithms\n",
    "# (i) SGD\n",
    "\n",
    "model_1 = SGDClassifier(loss = 'log', max_iter = 10000)\n",
    "model_1 = model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ii) Decision Tree\n",
    "\n",
    "model_2 = DecisionTreeClassifier(criterion = 'entropy')\n",
    "model_2 = model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(iii) Decision Stump of Depth 4 \n",
    "\n",
    "model_3 = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\n",
    "model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iv) Decision Stumps as Features\n",
    "\n",
    "# function below could randomly select half features from train and test data set.\n",
    "def random_choose_half(train_set, test_set):\n",
    "    # save the length of the train data\n",
    "    length = len(train_set)\n",
    "    # concatenate the train data and the test data\n",
    "    train_and_test = np.concatenate((train_set, test_set))\n",
    "    # change array to dataframe\n",
    "    data_df = pd.DataFrame(data = train_and_test)\n",
    "    # get the 50% data randomly\n",
    "    data_df_random = data_df.sample(frac = 0.5, axis = 1)\n",
    "    # convert the 2000*250 dataframe to a list whose length is 2000\n",
    "    X_list = []\n",
    "    X_list = data_df_random.values.tolist()\n",
    "    # convert the list to numpy array\n",
    "    X_train_array = np.array(X_list)[:length]\n",
    "    X_test_array = np.array(X_list)[length:]\n",
    "    return X_train_array, X_test_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the half-feature data set\n",
    "X_train_array, X_test_array = random_choose_half(X_train, X_test)\n",
    "\n",
    "label_list = []\n",
    "label_list_test = []\n",
    "\n",
    "# repeat 50 times for new feature matrix\n",
    "for i in range(0, 50):\n",
    "    \n",
    "    # concatenate the train data and test data together\n",
    "    whole_data = np.concatenate((X_train, X_test))\n",
    "    # change array to the dataframe\n",
    "    data_df = pd.DataFrame(data = whole_data)\n",
    "\n",
    "    # get the 50% data randomly\n",
    "    data_df_random = data_df.sample(frac = 0.5, axis = 1)\n",
    "\n",
    "    # convert the 2000*250 dataframe to a list whose length is 2000\n",
    "    X_list = []\n",
    "    X_list = data_df_random.values.tolist()\n",
    "\n",
    "    # convert the list to numpy array\n",
    "    X_train_array = np.array(X_list)[:2000]\n",
    "    X_test_array = np.array(X_list)[2000:]\n",
    "    y_array = np.array(y_train)\n",
    "    y_array_test = np.array(y_test)\n",
    "\n",
    "    # train a classfier for this 50% part of data \n",
    "    #model_4 = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 4)\n",
    "    model_4 = model_3.fit(X_train_array, y_array)\n",
    "    model_5 = model_3.fit(X_test_array, y_array_test)\n",
    "\n",
    "\n",
    "    # use this classifier to test the whole data set\n",
    "    label_list.append(model_4.predict(X_train_array)) \n",
    "    label_list_test.append(model_5.predict(X_test_array))\n",
    "\n",
    "# transpose the train array and test array to correct shape\n",
    "train_array = np.asarray(label_list)\n",
    "train_trans_array = train_array.transpose()\n",
    "\n",
    "test_array = np.asarray(label_list_test)\n",
    "test_trans_array = test_array.transpose()\n",
    "\n",
    "# train the final model by this 2000* 50 new dataset and the original label\n",
    "SGD_clf = linear_model.SGDClassifier()\n",
    "final_SGD = SGD_clf.fit(train_trans_array, y_train)\n",
    "\n",
    "# predict labels for test data\n",
    "final_SGD.predict(test_trans_array)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Crossing-Validation\n",
    "# Some useful functions and data\n",
    "\n",
    "# put all the data into the list\n",
    "list_trainX = [train_X0, train_X1, train_X2, train_X3, train_X4]\n",
    "list_trainy = [train_y0, train_y1, train_y2, train_y3, train_y4]\n",
    "list_heldoutX = [heldout_X0, heldout_X1, heldout_X2, heldout_X3, heldout_X4]\n",
    "list_heldouty = [heldout_y0, heldout_y1, heldout_y2, heldout_y3, heldout_y4]\n",
    "\n",
    "# function below which could get the accurancy \n",
    "def accuracy(train_X, train_y, heldout_X, heldout_y, classifier):\n",
    "\n",
    "    # train the classifier by using the train_set\n",
    "    classifier = classifier.fit(train_X, train_y)\n",
    "    \n",
    "    # get the label tested from predicting\n",
    "    true_y = classifier.predict(heldout_X)\n",
    "    \n",
    "    #calculate the accurancy of heldout data\n",
    "    accuracy_heldout = accuracy_score(heldout_y, true_y)  # accuracy = accuracy_score(prediction, labels_test)\n",
    "    \n",
    "    # calculate the accuracy of train data\n",
    "    true_y_train = classifier.predict(train_X)\n",
    "    accuracy_train = accuracy_score(train_y, true_y_train)\n",
    "    return accuracy_heldout, accuracy_train\n",
    "\n",
    "# function below which could get the average accurancy \n",
    "def average_accuracy(trainX, trainy, heldoutX, heldouty, classifier):\n",
    "    accuracy_train_SGD = []\n",
    "    accuracy_heldout_SGD = []\n",
    "    for i in range(0,5):\n",
    "        accuracy_train_SGD.append(accuracy(list_trainX[i], list_trainy[i], list_heldoutX[i], list_heldouty[i], classifier)[1])\n",
    "        accuracy_heldout_SGD.append(accuracy(list_trainX[i], list_trainy[i], list_heldoutX[i], list_heldouty[i], classifier)[0])        \n",
    "    return accuracy_train_SGD, accuracy_heldout_SGD\n",
    "\n",
    "# function below which could get the confidence_interval\n",
    "def confidence_interval(data):\n",
    "    confidence = 0.95\n",
    "\n",
    "    n = len(data)\n",
    "    m = mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    return start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy of SGD model:0.54775\n",
      "Average heldout accuracy of SGD model:0.5075000000000001\n",
      "Standard deviation of SGD_train:0.05894462443683901\n",
      "Standard deviation of SGD heldout0.016124515496597117\n",
      "Confidence intervals of SGD train:(0.4659217430023079, 0.6295782569976921)\n",
      "Confidence intervals of SGD heldout:(0.4851155839378935, 0.5298844160621066)\n"
     ]
    }
   ],
   "source": [
    "# for SGDClassifier\n",
    "accuracy_train_SGD, accuracy_heldout_SGD = average_accuracy(list_trainX, list_trainy, list_heldoutX, list_heldouty, classifier = linear_model.SGDClassifier())\n",
    "\n",
    "# for average train accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_train_SGD:\n",
    "    accur_sum += i\n",
    "accur_train_SGD = accur_sum / 5\n",
    "print(\"Average train accuracy of SGD model:\" + str(accur_train_SGD))\n",
    "\n",
    "# for average heldout accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_heldout_SGD:\n",
    "    accur_sum += i\n",
    "accur_heldout_SGD = accur_sum / 5\n",
    "print(\"Average heldout accuracy of SGD model:\" + str(accur_heldout_SGD))\n",
    "\n",
    "# for standard deviation of SGD\n",
    "std_SGD_train = np.std(accuracy_train_SGD, dtype=np.float64)\n",
    "print(\"Standard deviation of SGD_train:\" + str(std_SGD_train))\n",
    "std_SGD_heldout = np.std(accuracy_heldout_SGD, dtype=np.float64)\n",
    "print(\"Standard deviation of SGD heldout\" + str(std_SGD_heldout))\n",
    "\n",
    "# for confidence intervals of SGD\n",
    "confidence_interval_train_SGD = confidence_interval(accuracy_train_SGD)\n",
    "confidence_interval_heldout_SGD = confidence_interval(accuracy_heldout_SGD)\n",
    "print(\"Confidence intervals of SGD train:\" + str(confidence_interval_train_SGD))\n",
    "print(\"Confidence intervals of SGD heldout:\" + str(confidence_interval_heldout_SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy of DT model:1.0\n",
      "Average heldout accuracy of DT model:0.766\n",
      "Standard deviation of DT train:0.0\n",
      "Standard deviation of DT heldout:0.02452549693686147\n",
      "Confidence intervals of DT train:(1.0, 1.0)\n",
      "Confidence intervals of DT heldout:(0.7319531520385537, 0.8000468479614463)\n"
     ]
    }
   ],
   "source": [
    "#for DTClassifier\n",
    "accuracy_train_DT, accuracy_heldout_DT = average_accuracy(list_trainX, list_trainy, list_heldoutX, list_heldouty, classifier = DecisionTreeClassifier(criterion = 'entropy'))\n",
    "\n",
    "#for average train accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_train_DT:\n",
    "    accur_sum += i\n",
    "accur_train_DT = accur_sum / 5\n",
    "print(\"Average train accuracy of DT model:\" + str(accur_train_DT))\n",
    "\n",
    "# for average heldout accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_heldout_DT:\n",
    "    accur_sum += i\n",
    "accur_heldout_DT = accur_sum / 5\n",
    "print(\"Average heldout accuracy of DT model:\" + str(accur_heldout_DT))\n",
    "\n",
    "# for standard deviation\n",
    "std_DT_train = np.std(accuracy_train_DT, dtype=np.float64)\n",
    "print(\"Standard deviation of DT train:\" + str(std_DT_train))\n",
    "std_DT_heldout = np.std(accuracy_heldout_DT, dtype=np.float64)\n",
    "print(\"Standard deviation of DT heldout:\" + str(std_DT_heldout))\n",
    "\n",
    "# for confidence intervals \n",
    "confidence_interval_train_DT = confidence_interval(accuracy_train_DT)\n",
    "confidence_interval_heldout_DT = confidence_interval(accuracy_heldout_DT)\n",
    "print(\"Confidence intervals of DT train:\" + str(confidence_interval_train_DT))\n",
    "print(\"Confidence intervals of DT heldout:\" + str(confidence_interval_heldout_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy of DT4 model:0.78325\n",
      "Average heldout accuracy of DT4 model:0.7649999999999999\n",
      "Standard deviation of DT4 train:0.011801747751922166\n",
      "Standard deviation of DT4 heldout:0.016046806535881215\n",
      "Confidence intervals of DT4 train:(0.7668665476106983, 0.7996334523893017)\n",
      "Confidence intervals of DT4 heldout:(0.7427234612696982, 0.7872765387303016)\n"
     ]
    }
   ],
   "source": [
    "# for DT4Classifier\n",
    "accuracy_train_DT4, accuracy_heldout_DT4 = average_accuracy(list_trainX, list_trainy, list_heldoutX, list_heldouty, classifier = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4))\n",
    "\n",
    "#for average train accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_train_DT4:\n",
    "    accur_sum += i\n",
    "accur_train_DT4 = accur_sum / 5\n",
    "print(\"Average train accuracy of DT4 model:\" + str(accur_train_DT4))\n",
    "\n",
    "# for average heldout accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_heldout_DT4:\n",
    "    accur_sum += i\n",
    "accur_heldout_DT4 = accur_sum / 5\n",
    "print(\"Average heldout accuracy of DT4 model:\" + str(accur_heldout_DT4))\n",
    "\n",
    "# for standard deviation\n",
    "std_DT4_train = np.std(accuracy_train_DT4, dtype=np.float64)\n",
    "print(\"Standard deviation of DT4 train:\" + str(std_DT4_train))\n",
    "std_DT4_heldout = np.std(accuracy_heldout_DT4, dtype=np.float64)\n",
    "print(\"Standard deviation of DT4 heldout:\" + str(std_DT4_heldout))\n",
    "\n",
    "# for confidence intervals \n",
    "confidence_interval_train_DT4 = confidence_interval(accuracy_train_DT4)\n",
    "confidence_interval_heldout_DT4 = confidence_interval(accuracy_heldout_DT4)\n",
    "print(\"Confidence intervals of DT4 train:\" + str(confidence_interval_train_DT4))\n",
    "print(\"Confidence intervals of DT4 heldout:\" + str(confidence_interval_heldout_DT4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy of DSF:0.780625\n",
      "Average heldout accuracy of DSF:0.7475\n",
      "Standard deviation of DSF train:0.012443622864744807\n",
      "Standard deviation of DSF heldout:0.02302172886644268\n",
      "Confidence intervals of DSF train:(0.763350482103126, 0.7978995178968741)\n",
      "Confidence intervals of DSF heldout:(0.7155407167877872, 0.7794592832122129)\n"
     ]
    }
   ],
   "source": [
    "# for \"decision stumps as features\" Classifier (called \"DSF\" below)\n",
    "# new dataset\n",
    "train_X0_half, heldout_X0_half = random_choose_half(train_X0, heldout_X0)\n",
    "train_X1_half, heldout_X1_half = random_choose_half(train_X1, heldout_X1)\n",
    "train_X2_half, heldout_X2_half = random_choose_half(train_X2, heldout_X2)\n",
    "train_X3_half, heldout_X3_half = random_choose_half(train_X3, heldout_X3)\n",
    "train_X4_half, heldout_X4_half = random_choose_half(train_X4, heldout_X4)\n",
    "list_trainX = [train_X0_half, train_X1_half, train_X2_half, train_X3_half, train_X4_half]\n",
    "list_trainy = [train_y0, train_y1, train_y2, train_y3, train_y4]\n",
    "list_heldoutX = [heldout_X0_half, heldout_X1_half, heldout_X2_half, heldout_X3_half, heldout_X4_half]\n",
    "list_heldouty = [heldout_y0, heldout_y1, heldout_y2, heldout_y3, heldout_y4]\n",
    "\n",
    "\n",
    "accuracy_train_half, accuracy_heldout_half = average_accuracy(list_trainX, list_trainy, list_heldoutX, list_heldouty, classifier = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4))\n",
    "\n",
    "#for average train accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_train_half:\n",
    "    accur_sum += i\n",
    "accur_train_half = accur_sum / 5\n",
    "print(\"Average train accuracy of DSF:\" + str(accur_train_half))\n",
    "\n",
    "# for average heldout accuracy\n",
    "accur_sum = 0\n",
    "for i in accuracy_heldout_half:\n",
    "    accur_sum += i\n",
    "accur_heldout_half = accur_sum / 5\n",
    "print(\"Average heldout accuracy of DSF:\" + str(accur_heldout_half))\n",
    "\n",
    "# for standard deviation\n",
    "std_half_train = np.std(accuracy_train_half, dtype=np.float64)\n",
    "print(\"Standard deviation of DSF train:\" + str(std_half_train))\n",
    "std_half_heldout = np.std(accuracy_heldout_half, dtype=np.float64)\n",
    "print(\"Standard deviation of DSF heldout:\" + str(std_half_heldout))\n",
    "\n",
    "# for confidence intervals\n",
    "confidence_interval_train_half = confidence_interval(accuracy_train_half)\n",
    "confidence_interval_heldout_half = confidence_interval(accuracy_heldout_half)\n",
    "print(\"Confidence intervals of DSF train:\" + str(confidence_interval_train_half))\n",
    "print(\"Confidence intervals of DSF heldout:\" + str(confidence_interval_heldout_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of SGD model for test data: 0.54\n",
      "accuracy of DT model for test data: 0.775\n",
      "accuracy of DT4 model for test data: 0.7516666666666667\n",
      "accuracy of DSF model for test data: 0.5033333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: Testing\n",
    "# for SGD \n",
    "acc_test_SGD = accuracy(X_train, y_train, X_test, y_test, model_1)[0]\n",
    "print(\"accuracy of SGD model for test data: \" + str(acc_test_SGD))\n",
    "\n",
    "# for DT\n",
    "acc_test_DT = accuracy(X_train, y_train, X_test, y_test, model_2)[0]\n",
    "print(\"accuracy of DT model for test data: \" + str(acc_test_DT))\n",
    "\n",
    "# for DT4\n",
    "acc_test_DT4 = accuracy(X_train, y_train, X_test, y_test, model_3)[0]\n",
    "print(\"accuracy of DT4 model for test data: \" + str(acc_test_DT4))\n",
    "\n",
    "# for half_model\n",
    "acc_test_DSF = accuracy(X_train, y_train, X_test, y_test, final_SGD)[0]\n",
    "print(\"accuracy of DSF model for test data: \" + str(acc_test_DSF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_results(accur_train_SGD, accur_heldout_SGD, std_SGD_train, std_SGD_heldout, acc_test_SGD, accur_DT_train,\n",
    "                 accur_DT_heldout, std_DT_train, std_DT_heldout, acc_test_DT, accur_DT4_train, accur_DT4_heldout,\n",
    "                 std_DT4_train, std_DT4_heldout, acc_test_DT4, accur_train_half, accur_heldout_half, std_half_train, std_half_heldout, acc_test_DSF):\n",
    "    \"\"\"\n",
    "    Plots the final results from problem 2. For each of the 4 classifiers, pass\n",
    "    the training accuracy, training standard deviation, held-out accuracy, held-out\n",
    "    standard deviation, and testing accuracy.\n",
    "\n",
    "    Although it should not be necessary, feel free to edit this method.\n",
    "    \"\"\"\n",
    "    train_x_pos = [0, 4, 8, 12]\n",
    "    cv_x_pos = [1, 5, 9, 13]\n",
    "    test_x_pos = [2, 6, 10, 14]\n",
    "    ticks = cv_x_pos\n",
    "\n",
    "    labels = ['sgd', 'dt', 'dt4', 'stumps (2000 x 50)']\n",
    "\n",
    "    train_accs = [accur_train_SGD, accur_DT_train, accur_DT4_train, accur_train_half]\n",
    "    train_errors = [std_SGD_heldout, std_DT_train, std_DT4_train, std_half_train]\n",
    "\n",
    "    cv_accs = [accur_heldout_SGD, accur_DT_heldout, accur_DT4_heldout, accur_heldout_half]\n",
    "    cv_errors = [std_SGD_heldout, std_DT_heldout, std_DT4_heldout, std_half_heldout]\n",
    "\n",
    "    test_accs = [acc_test_SGD, acc_test_DT, acc_test_DT4, acc_test_DSF]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(train_x_pos, train_accs, yerr=train_errors, align='center', alpha=0.5, ecolor='black', capsize=10, label='train')\n",
    "    ax.bar(cv_x_pos, cv_accs, yerr=cv_errors, align='center', alpha=0.5, ecolor='black', capsize=10, label='held-out')\n",
    "    ax.bar(test_x_pos, test_accs, align='center', alpha=0.5, capsize=10, label='test')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title('Models')\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(accur_train_SGD, accur_heldout_SGD, std_SGD_train, std_SGD_heldout, acc_test_SGD, accur_train_DT,\n",
    "                 accur_heldout_DT, std_DT_train, std_DT_heldout, acc_test_DT, accur_train_DT4, accur_heldout_DT4,\n",
    "                 std_DT4_train, std_DT4_heldout, acc_test_DT4, accur_train_half, accur_heldout_half, std_half_train, std_half_heldout, acc_test_DSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# When you turn this function in to Gradescope, it is easiest to copy and paste this cell to a new python file called hw1.py\n",
    "# and upload that file instead of the full Jupyter Notebook code (which will cause problems for Gradescope)\n",
    "def compute_features(names):\n",
    "    \"\"\"\n",
    "    Given a list of names of length N, return a numpy matrix of shape (N, 260)\n",
    "    with the features described in problem 2b of the homework assignment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    names: A list of strings\n",
    "        The names to featurize, e.g. [\"albert einstein\", \"marie curie\"]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.array:\n",
    "        A numpy array of shape (N, 260)\n",
    "    \"\"\"\n",
    "    N = len(names)\n",
    "    a= np.zeros((N, 260))\n",
    "    name_order = 0\n",
    "    \n",
    "    for name in names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        last_name = name.split(\" \")[1]\n",
    "        first_len = len(first_name)\n",
    "        last_len = len(last_name)\n",
    "        \n",
    "        char_order = 0\n",
    "        if first_len >= 5 and last_len >= 5:\n",
    "            name_total = first_name[0:5] + last_name[0:5]\n",
    "        elif first_len < 5 and last_len >= 5:\n",
    "            name_total = first_name[0:first_len] + \"0\"*(5 - first_len) + last_name[0:5]\n",
    "        elif first_len >= 5 and last_len < 5:\n",
    "            name_total = first_name[0:5] + last_name[0:5] + \"0\"*(5 - last_len) \n",
    "        else:\n",
    "            name_total = first_name[0: first_len] + \"0\" * (5 - first_len) + last_name[0:last_len] + \"0\" *(5 - last_len)\n",
    "        \n",
    "        \n",
    "        for char in name_total:\n",
    "            if char.isupper():\n",
    "                insert_one_place = ord(char) - 64\n",
    "                a[name_order][26 * char_order + insert_one_place - 1] = 1\n",
    "                \n",
    "            elif char.islower():\n",
    "                insert_one_place = ord(char) - 96\n",
    "                a[name_order][26 * char_order + insert_one_place - 1] = 1\n",
    "            else:\n",
    "                pass\n",
    "          \n",
    "            char_order += 1\n",
    "     \n",
    "        name_order += 1\n",
    "    return a\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the required data for Badges Game\n",
    "with open('Badges/train.names.txt') as fin:\n",
    "    train_name = fin.readlines()\n",
    "for i in train_name:\n",
    "    i = i.strip()\n",
    "    \n",
    "train_label = np.load('Badges/train.labels.npy')\n",
    "\n",
    "with open('Badges/test.names.txt') as fin:\n",
    "    test_name = fin.readlines()\n",
    "\n",
    "test_label = np.load('Badges/test.labels.npy')\n",
    "with open('Badges/hidden-test.names.txt') as fin:\n",
    "    hidden_name = fin.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: accuracy for test data: 0.631 for train data: 0.747\n",
      "DT: accuracy for test data: 0.623 for train data: 1.0\n",
      "DT4: accuracy for test data: 0.65 for train data: 0.677\n",
      "DSF: accuracy for test data: 0.641 for train data: 0.756\n"
     ]
    }
   ],
   "source": [
    "# get the feature extraction\n",
    "train_data = compute_features(train_name)\n",
    "test_data = compute_features(test_name)\n",
    "hidden_data = compute_features(hidden_name)\n",
    "\n",
    "# SGD\n",
    "accuracy_test_1, accuracy_train_1 = accuracy(train_data, train_label, test_data, test_label, model_1)\n",
    "print(\"SGD: accuracy for test data: \" + str(accuracy_test_1) + \" for train data: \" + str(accuracy_train_1))\n",
    "\n",
    "# DT\n",
    "accuracy_test_2, accuracy_train_2 = accuracy(train_data, train_label, test_data, test_label, model_2)\n",
    "print(\"DT: accuracy for test data: \" + str(accuracy_test_2) + \" for train data: \" + str(accuracy_train_2))\n",
    "\n",
    "# DT4\n",
    "accuracy_test_3, accuracy_train_3 = accuracy(train_data, train_label, test_data, test_label, model_3)\n",
    "print(\"DT4: accuracy for test data: \" + str(accuracy_test_3) + \" for train data: \" + str(accuracy_train_3))\n",
    "\n",
    "# DSF\n",
    "accuracy_test_4, accuracy_train_4 = accuracy(train_data, train_label, test_data, test_label, final_SGD)\n",
    "print(\"DSF: accuracy for test data: \" + str(accuracy_test_4) + \" for train data: \" + str(accuracy_train_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra credit\n",
    "def compute_new_features(names):\n",
    "    \"\"\"\n",
    "    Given a list of names of length N, return a numpy matrix of shape (N, 20)\n",
    "    with the features about whether this letter is vowel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    names: A list of strings\n",
    "        The names to featurize, e.g. [\"albert einstein\", \"marie curie\"]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.array:\n",
    "        A numpy array of shape (N, 20)\n",
    "    \"\"\"\n",
    "    N = len(names)\n",
    "    a= np.zeros((N, 20))\n",
    "    name_order = 0\n",
    "    vowels_list = ['a', 'e', 'i', 'o', 'u']\n",
    "    \n",
    "    for name in names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        last_name = name.split(\" \")[1]\n",
    "        first_len = len(first_name)\n",
    "        last_len = len(last_name)\n",
    "        \n",
    "        char_order = 0\n",
    "        if first_len >= 10 and last_len >= 10:\n",
    "            name_total = first_name[0:10] + last_name[0:10]\n",
    "        elif first_len < 10 and last_len >= 10:\n",
    "            name_total = first_name[0:first_len] + \"0\"*(10 - first_len) + last_name[0:10]\n",
    "        elif first_len >= 10 and last_len < 10:\n",
    "            name_total = first_name[0:10] + last_name[0:5] + \"0\"*(10 - last_len) \n",
    "        else:\n",
    "            name_total = first_name[0: first_len] + \"0\" * (10 - first_len) + last_name[0:last_len] + \"0\" *(10 - last_len)\n",
    "        \n",
    "        \n",
    "        for char in name_total:\n",
    "            if char in vowels_list:\n",
    "                a[name_order][char_order] = 1\n",
    "          \n",
    "            else:\n",
    "                a[name_order][char_order] = 0\n",
    "          \n",
    "            char_order += 1\n",
    "     \n",
    "        name_order += 1\n",
    "    return a\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: accuracy for test data: 0.698 for train data: 0.638\n",
      "DT: accuracy for test data: 0.828 for train data: 1.0\n",
      "DT4: accuracy for test data: 0.65 for train data: 0.66\n",
      "DSF: accuracy for test data: 0.664 for train data: 0.672\n"
     ]
    }
   ],
   "source": [
    "# get the feature extraction\n",
    "train_data = compute_new_features(train_name)\n",
    "test_data = compute_new_features(test_name)\n",
    "hidden_data = compute_new_features(hidden_name)\n",
    "\n",
    "# ???? need a new model or base on trained model ?????\n",
    "# SGD\n",
    "accuracy_test_1, accuracy_train_1 = accuracy(train_data, train_label, test_data, test_label, model_1)\n",
    "print(\"SGD: accuracy for test data: \" + str(accuracy_test_1) + \" for train data: \" + str(accuracy_train_1))\n",
    "\n",
    "# DT\n",
    "accuracy_test_2, accuracy_train_2 = accuracy(train_data, train_label, test_data, test_label, model_2)\n",
    "print(\"DT: accuracy for test data: \" + str(accuracy_test_2) + \" for train data: \" + str(accuracy_train_2))\n",
    "\n",
    "# DT4\n",
    "accuracy_test_3, accuracy_train_3 = accuracy(train_data, train_label, test_data, test_label, model_3)\n",
    "print(\"DT4: accuracy for test data: \" + str(accuracy_test_3) + \" for train data: \" + str(accuracy_train_3))\n",
    "\n",
    "# DSF\n",
    "accuracy_test_4, accuracy_train_4 = accuracy(train_data, train_label, test_data, test_label, final_SGD)\n",
    "print(\"DSF: accuracy for test data: \" + str(accuracy_test_4) + \" for train data: \" + str(accuracy_train_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
      " 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
      " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
      " 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "#  extra credit\n",
    "\n",
    "hidden_label = model_2.predict(hidden_data)\n",
    "print(hidden_label)\n",
    "\n",
    "import numpy as np\n",
    "np.savetxt(\"labels.txt\", hidden_label, fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
